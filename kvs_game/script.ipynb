{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os \n",
    "import cv2\n",
    "import skimage\n",
    "from PIL import Image\n",
    "from ultralytics import YOLO\n",
    "from matplotlib import pyplot as plt\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Homography Matrix using keypoints coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2940 1912\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[   0.018917,     0.12276,     -250.61],\n",
       "       [  -0.097773,     -0.1908,      253.59],\n",
       "       [-1.1254e-05,  -0.0012991,           1]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the original image to get its dimensions\n",
    "original_image = cv2.imread('kvs.png')\n",
    "original_height, original_width = original_image.shape[:2]\n",
    "\n",
    "# New dimensions to which the frame is resized\n",
    "# window_width = 854\n",
    "# window_height = 480\n",
    "\n",
    "keypoints_path = \"vocational_keypoints.json\"\n",
    "# keypoints_path = \"trainingW.json\"\n",
    "\n",
    "window_width = original_width\n",
    "window_height = original_height\n",
    "print(window_width, window_height)\n",
    "\n",
    "# Calculate scaling factors\n",
    "x_scale = window_width / original_width\n",
    "y_scale = window_height / original_height\n",
    "\n",
    "# Load coordinates from JSON file\n",
    "with open(f'coordinates/{keypoints_path}', 'r') as f:\n",
    "    vocational_keypoints = json.load(f)\n",
    "\n",
    "# Update coordinates with the scaling factor\n",
    "for key, value in vocational_keypoints.items():\n",
    "    if value:  # Check if the list is not empty\n",
    "        x, y = value\n",
    "        new_x = int(x * x_scale)\n",
    "        new_y = int(y * y_scale)\n",
    "        vocational_keypoints[key] = [new_x, new_y]\n",
    "\n",
    "# Optionally, save the updated coordinates back to a file\n",
    "with open('coordinates/updated.json', 'w') as f:\n",
    "    json.dump(vocational_keypoints, f, indent=2)\n",
    "\n",
    "with open('coordinates/updated.json', 'r') as f:\n",
    "    vocational_keypoints = json.load(f)\n",
    "\n",
    "with open('coordinates/map_keypoints.json', 'r') as f:\n",
    "    map_keypoints = json.load(f)\n",
    "\n",
    "# Filter out keypoints that are not visible\n",
    "image_points = []\n",
    "map_points = []\n",
    "label_points = {}\n",
    "\n",
    "i = 0\n",
    "\n",
    "for key in vocational_keypoints:\n",
    "    if vocational_keypoints[key] and map_keypoints[key]:  # Ensure keypoints are visible on both the frame and the map\n",
    "        image_points.append(vocational_keypoints[key])\n",
    "        map_points.append(map_keypoints[key])\n",
    "        label_points[key] = i \n",
    "        i+=1\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "image_points = np.array(image_points, dtype=np.float32)\n",
    "map_points = np.array(map_points, dtype=np.float32)\n",
    "\n",
    "# Calculate the homography matrix\n",
    "H, status = cv2.findHomography(image_points, map_points, cv2.RANSAC, 5.0)\n",
    "H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing video and entering team colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = './videos/KVS_H1_TEST.mp4'\n",
    "# video_path = './videos/TrainingW_TEST.mp4'\n",
    "\n",
    "tac_map = cv2.imread('./map.jpg')\n",
    "\n",
    "# Define team colors\n",
    "nbr_team_colors = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colors_dic = {\n",
    "#     \"Purple\":[(105,105,160), (105,105,160)], # DKU Colors (Players kit color, GK kit color)\n",
    "#     \"Yellow\":[(200,200,100), (200,200,100)] # KVS Colors (Players kit color, GK kit color)\n",
    "# }\n",
    "# colors_list = colors_dic[\"Purple\"]+colors_dic[\"Yellow\"] # Define color list to be used for detected player team prediction\n",
    "\n",
    "colors_dic = {\n",
    "    \"DKU\":[(200,200,200), (115,144,98)], # DKU Colors (Players kit color, GK kit color)\n",
    "    \"KVS\":[(120,52,47), (47,103,43)] # KVS Colors (Players kit color, GK kit color)\n",
    "}\n",
    "colors_list = colors_dic[\"DKU\"]+colors_dic[\"KVS\"] # Define color list to be used for detected player team prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_list_lab = [skimage.color.rgb2lab([i/255 for i in c]) for c in colors_list] # Converting color_list to L*a*b* space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading weights and setting labels depending on the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Load the model\n",
    "model = torch.load('weights/best.pt')\n",
    "# Print out the model details\n",
    "print(model.keys())\n",
    "model['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the YOLOv8 players detection model\n",
    "\n",
    "# model_players = YOLO(\"weights/best.pt\")\n",
    "# labels_dic = {\n",
    "#     0:\"player\",\n",
    "#     1:\"referee\",\n",
    "#     2:\"ball\",\n",
    "# }\n",
    "\n",
    "model_players = YOLO(\"weights/best_nafie.pt\")\n",
    "labels_dic = {\n",
    "    0:\"Player team left\",\n",
    "    1:\"Player team right\",\n",
    "    2:\"Goalkeeper team left\",\n",
    "    3:\"Goalkeeper team right\",\n",
    "    4:\"Ball\",\n",
    "    5:\"Main referee\",\n",
    "    6:\"Side referee\",\n",
    "    7:\"Staff members\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open video file\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Initialize frame counter\n",
    "frame_nbr = 0\n",
    "\n",
    "# Set keypoints average displacement tolerance level (in pixels) [set to -1 to always update homography matrix]\n",
    "keypoints_displacement_mean_tol = 10\n",
    "\n",
    "# Set confidence thresholds for players and field keypoints detections\n",
    "player_model_conf_thresh = 0.50\n",
    "\n",
    "# Set variable to record the time when we processed last frame \n",
    "prev_frame_time = 0\n",
    "# Set variable to record the time at which we processed current frame \n",
    "new_frame_time = 0\n",
    "\n",
    "# Store the ball track history\n",
    "ball_track_history = {'src':[],\n",
    "                      'dst':[]\n",
    "}\n",
    "\n",
    "# Count consecutive frames with no ball detected\n",
    "nbr_frames_no_ball = 0\n",
    "# Threshold for number of frames with no ball to reset ball track (frames)\n",
    "nbr_frames_no_ball_thresh = 30\n",
    "# Distance threshold for ball tracking (pixels)\n",
    "ball_track_dist_thresh = 100\n",
    "# Maximum ball track length (detections)\n",
    "max_track_length = 35\n",
    "# Initialize counters and storage for class detections\n",
    "class_detection_counts = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_save_path = 'saved_frames'\n",
    "os.makedirs(frame_save_path, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "\n",
    "# Loop through the video frames\n",
    "while cap.isOpened():\n",
    "\n",
    "    # Read a frame from the video\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        print(\"Failed to read frame or end of video reached.\")\n",
    "        break  # Exit the loop if no frame is read or end of video is reached\n",
    "\n",
    "    frame = cv2.resize(frame, (window_width, window_height))\n",
    "\n",
    "    # Save each frame to the folder\n",
    "    frame_nbr += 1\n",
    "    \n",
    "    # Save number of detections per frame\n",
    "    current_frame_class_counts = {}\n",
    "\n",
    "    # Reset tactical map image for each new frame\n",
    "    tac_map_copy = tac_map.copy()\n",
    "    # Define the transparency factor (alpha)\n",
    "    alpha = 0.5  # 50% transparency\n",
    "    beta = (1.0 - alpha)  # Weight of the second array elements\n",
    "\n",
    "    # Create a black background of the same size as the tactical map\n",
    "    background = np.zeros_like(tac_map_copy)\n",
    "\n",
    "    # Blend the tactical map with the black background\n",
    "    tac_map_copy = cv2.addWeighted(tac_map_copy, alpha, background, beta, 0)\n",
    "\n",
    "    # Reset ball tracks\n",
    "    if nbr_frames_no_ball>nbr_frames_no_ball_thresh:\n",
    "            ball_track_history['dst'] = []\n",
    "            ball_track_history['src'] = []\n",
    "\n",
    "    # Process the frame if it was successfuly read\n",
    "    if success:\n",
    "        \n",
    "        #################### Part 1 ####################\n",
    "        # Object Detection & Coordiante Transofrmation #\n",
    "        ################################################\n",
    "\n",
    "        # Run YOLOv8 players inference on the frame\n",
    "        results_players = model_players(frame, conf=player_model_conf_thresh)\n",
    "\n",
    "        ## Extract detections information\n",
    "        bboxes_p = results_players[0].boxes.xyxy.cpu().numpy()                # Detected players, referees and ball (x,y,x,y) bounding boxes\n",
    "        bboxes_p_c = results_players[0].boxes.xywh.cpu().numpy()              # Detected players, referees and ball (x,y,w,h) bounding boxes    \n",
    "        labels_p = list(results_players[0].boxes.cls.cpu().numpy())           # Detected players, referees and ball labels list\n",
    "        confs_p = list(results_players[0].boxes.conf.cpu().numpy())           # Detected players, referees and ball confidence level\n",
    "                                \n",
    "        bboxes_p_c_0 = bboxes_p_c[[i==0 for i in labels_p],:]                 # Get bounding boxes information (x,y,w,h) of detected players (label 0)\n",
    "        bboxes_p_c_2 = bboxes_p_c[[i==2 for i in labels_p],:]                 # Get bounding boxes information (x,y,w,h) of detected ball(s) (label 2)\n",
    "\n",
    "        # Get coordinates of detected players on frame (x_cencter, y_center+h/2)\n",
    "        detected_ppos_src_pts = bboxes_p_c_0[:,:2]  + np.array([[0]*bboxes_p_c_0.shape[0], bboxes_p_c_0[:,3]/2]).transpose()\n",
    "        # Get coordinates of the first detected ball (x_center, y_center)\n",
    "        detected_ball_src_pos = bboxes_p_c_2[0,:2] if bboxes_p_c_2.shape[0]>0 else None\n",
    "\n",
    "        # Transform players coordinates from frame plane to tactical map plance using the calculated Homography matrix\n",
    "        pred_dst_pts = []                                                     # Initialize players tactical map coordiantes list\n",
    "        for pt in detected_ppos_src_pts:     \n",
    "            # print(\"Coor:\", pt)\n",
    "            pt = np.append(np.array(pt), np.array([1]), axis=0)               # Covert to homogeneous coordiantes\n",
    "            dest_point = np.matmul(H, np.transpose(pt))                       # Apply homography transofrmation\n",
    "            dest_point = dest_point/dest_point[2]           \n",
    "            # print(\"map:\", list(np.transpose(dest_point)[:2]))\n",
    "            pred_dst_pts.append(list(np.transpose(dest_point)[:2]))           # Update players tactical map coordiantes list\n",
    "        pred_dst_pts = np.array(pred_dst_pts)\n",
    "\n",
    "        # Transform ball coordinates from frame plane to tactical map plane using the calculated Homography matrix\n",
    "        if detected_ball_src_pos is not None:\n",
    "            pt = np.append(np.array(detected_ball_src_pos), np.array([1]), axis=0)\n",
    "            dest_point = np.matmul(H, np.transpose(pt))\n",
    "            dest_point = dest_point/dest_point[2]\n",
    "            detected_ball_dst_pos = np.transpose(dest_point)\n",
    "\n",
    "            # Update track ball position history\n",
    "            if len(ball_track_history['src'])>0 :\n",
    "                if np.linalg.norm(detected_ball_src_pos-ball_track_history['src'][-1])<ball_track_dist_thresh:\n",
    "                    ball_track_history['src'].append((int(detected_ball_src_pos[0]), int(detected_ball_src_pos[1])))\n",
    "                    ball_track_history['dst'].append((int(detected_ball_dst_pos[0]), int(detected_ball_dst_pos[1])))\n",
    "                else:\n",
    "                    ball_track_history['src']=[(int(detected_ball_src_pos[0]), int(detected_ball_src_pos[1]))]\n",
    "                    ball_track_history['dst']=[(int(detected_ball_dst_pos[0]), int(detected_ball_dst_pos[1]))]\n",
    "            else:\n",
    "                ball_track_history['src'].append((int(detected_ball_src_pos[0]), int(detected_ball_src_pos[1])))\n",
    "                ball_track_history['dst'].append((int(detected_ball_dst_pos[0]), int(detected_ball_dst_pos[1])))\n",
    "                \n",
    "        # Remove oldest tracked ball postion if track exceedes threshold        \n",
    "        if len(ball_track_history) > max_track_length:\n",
    "                ball_track_history['src'].pop(0)\n",
    "                ball_track_history['dst'].pop(0)\n",
    "\n",
    "        # After detection, populate the count for each detected class in the current frame\n",
    "        for label in labels_p:\n",
    "            if label in current_frame_class_counts:\n",
    "                current_frame_class_counts[label] += 1\n",
    "            else:\n",
    "                current_frame_class_counts[label] = 1\n",
    "\n",
    "        # Update the total detections for each class\n",
    "        for label, count in current_frame_class_counts.items():\n",
    "            if label in class_detection_counts:\n",
    "                class_detection_counts[label] += count\n",
    "            else:\n",
    "                class_detection_counts[label] = count\n",
    "\n",
    "        ######### Part 2 ########## \n",
    "        # Players Team Prediction #\n",
    "        ###########################\n",
    "\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)                    # Convert frame to RGB\n",
    "        obj_palette_list = []                                                 # Initialize players color palette list\n",
    "        palette_interval = (0,5)                                              # Color interval to extract from dominant colors palette (1rd to 5th color)\n",
    "        annotated_frame = frame                                               # Create annotated frame \n",
    "\n",
    "        ## Loop over detected players (label 0) and extract dominant colors palette based on defined interval\n",
    "        for i, j in enumerate(list(results_players[0].boxes.cls.cpu().numpy())):\n",
    "            if int(j) == 0:\n",
    "                bbox = results_players[0].boxes.xyxy.cpu().numpy()[i,:]                         # Get bbox info (x,y,x,y)\n",
    "                obj_img = frame_rgb[int(bbox[1]):int(bbox[3]), int(bbox[0]):int(bbox[2])]       # Crop bbox out of the frame\n",
    "                obj_img_w, obj_img_h = obj_img.shape[1], obj_img.shape[0]\n",
    "\n",
    "                center_filter_x1 = np.max([(obj_img_w//2)-(obj_img_w//5), 1])\n",
    "                center_filter_x2 = (obj_img_w//2)+(obj_img_w//5)\n",
    "                center_filter_y1 = np.max([(obj_img_h//3)-(obj_img_h//5), 1])\n",
    "                center_filter_y2 = (obj_img_h//3)+(obj_img_h//5)\n",
    "                center_filter = obj_img[center_filter_y1:center_filter_y2, \n",
    "                                        center_filter_x1:center_filter_x2]\n",
    "\n",
    "                # obj_pil_img = Image.fromarray(np.uint8(center_filter))                        # Convert to pillow image\n",
    "                obj_pil_img = Image.fromarray(np.uint8(center_filter))                          # Convert to pillow image\n",
    "\n",
    "                reduced = obj_pil_img.convert(\"P\", palette=Image.Palette.WEB)                   # Convert to web palette (216 colors)\n",
    "                palette = reduced.getpalette()                                                  # Get palette as [r,g,b,r,g,b,...]\n",
    "                palette = [palette[3*n:3*n+3] for n in range(256)]                              # Group 3 by 3 = [[r,g,b],[r,g,b],...]\n",
    "                color_count = [(n, palette[m]) for n,m in reduced.getcolors()]                  # Create list of palette colors with their frequency\n",
    "                RGB_df = pd.DataFrame(color_count, columns = ['cnt', 'RGB']).sort_values(       # Create dataframe based on defined palette interval\n",
    "                                      by = 'cnt', ascending = False).iloc[\n",
    "                                          palette_interval[0]:palette_interval[1],:]\n",
    "                palette = list(RGB_df.RGB)                                                      # Convert palette to list (for faster processing)\n",
    "                annotated_frame = cv2.rectangle(annotated_frame,                                # Add center filter bbox annotations\n",
    "                                                (int(bbox[0])+center_filter_x1, \n",
    "                                                 int(bbox[1])+ center_filter_y1),  \n",
    "                                                (int(bbox[0])+center_filter_x2, \n",
    "                                                 int(bbox[1])+center_filter_y2), (0,0,0), 2)\n",
    "                \n",
    "                # Update detected players color palette list\n",
    "                obj_palette_list.append(palette)\n",
    "        \n",
    "        ## Calculate distances between each color from every detected player color palette and the predefined teams colors\n",
    "        players_distance_features = []\n",
    "        # Loop over detected players extracted color palettes\n",
    "        for palette in obj_palette_list:\n",
    "            palette_distance = []\n",
    "            palette_lab = [skimage.color.rgb2lab([i/255 for i in color]) for color in palette]  # Convert colors to L*a*b* space\n",
    "            # Loop over colors in palette\n",
    "            for color in palette_lab:\n",
    "                distance_list = []\n",
    "                # Loop over predefined list of teams colors\n",
    "                for c in color_list_lab:\n",
    "                    #distance = np.linalg.norm([i/255 - j/255 for i,j in zip(color,c)])\n",
    "                    distance = skimage.color.deltaE_cie76(color, c)                             # Calculate Euclidean distance in Lab color space\n",
    "                    distance_list.append(distance)                                              # Update distance list for current color\n",
    "                palette_distance.append(distance_list)                                          # Update distance list for current palette\n",
    "            players_distance_features.append(palette_distance)                                  # Update distance features list\n",
    "\n",
    "        ## Predict detected players teams based on distance features\n",
    "        players_teams_list = []\n",
    "        # Loop over players distance features\n",
    "        for distance_feats in players_distance_features:\n",
    "            vote_list=[]\n",
    "            # Loop over distances for each color \n",
    "            for dist_list in distance_feats:\n",
    "                team_idx = dist_list.index(min(dist_list))//nbr_team_colors                     # Assign team index for current color based on min distance\n",
    "                vote_list.append(team_idx)                                                      # Update vote voting list with current color team prediction\n",
    "            players_teams_list.append(max(vote_list, key=vote_list.count))                      # Predict current player team by vote counting\n",
    "\n",
    "        #################### Part 3 #####################\n",
    "        # Updated Frame & Tactical Map With Annotations #\n",
    "        #################################################\n",
    "\n",
    "        ball_color_bgr = (0,0,255)                                                              # Color (GBR) for ball annotation on tactical map\n",
    "        j=0                                                                                     # Initializing counter of detected players\n",
    "        palette_box_size = 5                                                                    # Set color box size in pixels (for display)\n",
    "\n",
    "        # Loop over all detected object by players detection model\n",
    "        for i in range(bboxes_p.shape[0]):\n",
    "            conf = confs_p[i]                                                                   # Get confidence of current detected object\n",
    "            if labels_p[i]==0:                                                                  # Display annotation for detected players (label 0)\n",
    "                # Display extracted color palette for each detected player\n",
    "                palette = obj_palette_list[j]                                                               # Get color palette of the detected player\n",
    "                for k, c in enumerate(palette):\n",
    "                    c_bgr = c[::-1]                                                                         # Convert color to BGR\n",
    "                    annotated_frame = cv2.rectangle(annotated_frame, (int(bboxes_p[i,2])+3,                 # Add color palette annotation on frame\n",
    "                                                            int(bboxes_p[i,1])+k*palette_box_size),\n",
    "                                                            (int(bboxes_p[i,2])+palette_box_size,\n",
    "                                                            int(bboxes_p[i,1])+(palette_box_size)*(k+1)),\n",
    "                                                              c_bgr, -1)\n",
    "\n",
    "                team_name = list(colors_dic.keys())[players_teams_list[j]]                                  # Get detected player team prediction\n",
    "                color_rgb = colors_dic[team_name][0]                                                        # Get detected player team color\n",
    "                color_bgr = color_rgb[::-1]                                                                 # Convert color to bgr\n",
    "\n",
    "                annotated_frame = cv2.rectangle(annotated_frame, (int(bboxes_p[i,0]), int(bboxes_p[i,1])),  # Add bbox annotations with team colors\n",
    "                                                (int(bboxes_p[i,2]), int(bboxes_p[i,3])), color_bgr, 1)\n",
    "                \n",
    "                cv2.putText(annotated_frame, team_name + f\" {conf:.2f}\",                                    # Add team name annotations\n",
    "                             (int(bboxes_p[i,0]), int(bboxes_p[i,1])-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
    "                               color_bgr, 2)\n",
    "                \n",
    "                if color_bgr == (255, 255, 255):\n",
    "                    color_bgr = (0,0,0) \n",
    "                \n",
    "                # Add tactical map player postion color coded annotation\n",
    "                tac_map_copy = cv2.circle(tac_map_copy, (abs(int(pred_dst_pts[j][0])),abs(int(pred_dst_pts[j][1]))),\n",
    "                                          radius=5, color=color_bgr, thickness=-1)          \n",
    "                j+=1                                                                                        # Update players counter\n",
    "                \n",
    "            else:                                                                                            # Display annotation for other detections (label 1, 2)\n",
    "                annotated_frame = cv2.rectangle(annotated_frame, (int(bboxes_p[i,0]), int(bboxes_p[i,1])),  \n",
    "                                                 (int(bboxes_p[i,2]), int(bboxes_p[i,3])), (255,255,255), 4) # Add white colored bbox annotations\n",
    "                cv2.putText(annotated_frame, labels_dic[int(labels_p[i])] + f\" {conf:.2f}\",\n",
    "                            (int(bboxes_p[i,0]), int(bboxes_p[i,1])-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
    "                              (255,255,255), 2)                                                              # Add white colored label text annotations\n",
    "\n",
    "                # Add tactical map ball postion annotation if detected\n",
    "                if detected_ball_src_pos is not None:\n",
    "                    tac_map_copy = cv2.circle(tac_map_copy, (int(detected_ball_dst_pos[0]), \n",
    "                                                             int(detected_ball_dst_pos[1])), radius=5, \n",
    "                                                             color=ball_color_bgr, thickness=3)\n",
    "        \n",
    "        # Plot the ball tracks on tactical map\n",
    "        if len(ball_track_history['src'])>0:\n",
    "            points = np.hstack(ball_track_history['dst']).astype(np.int32).reshape((-1, 1, 2))\n",
    "            tac_map_copy = cv2.polylines(tac_map_copy, [points], isClosed=False, color=(0, 0, 100), thickness=2)\n",
    "        \n",
    "        # Combine annotated frame and tactical map in one image with colored border separation\n",
    "        border_color = [255,255,255]                                                                        # Set border color (BGR)\n",
    "        annotated_frame=cv2.copyMakeBorder(annotated_frame, 40, 10, 10, 10,                                 # Add borders to annotated frame\n",
    "                                            cv2.BORDER_CONSTANT, value=border_color)\n",
    "        tac_map_copy = cv2.rotate(tac_map_copy, cv2.ROTATE_90_COUNTERCLOCKWISE)                             # 90 degrees rotation\n",
    "\n",
    "        # Calculate the position to start the overlay (top right corner)\n",
    "        start_y = 50\n",
    "        start_x = annotated_frame.shape[1] - tac_map_copy.shape[1] - 50 # Start at the right edge of the annotated_frame\n",
    "\n",
    "        annotated_frame[start_y:start_y + tac_map_copy.shape[0], start_x:start_x + tac_map_copy.shape[1]] = tac_map_copy\n",
    "\n",
    "        final_img = annotated_frame\n",
    "\n",
    "        ## Add info annotation\n",
    "        cv2.putText(final_img, \"Press 'p' to pause & 'q' to quit\", (300,30), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,0,0), 2)\n",
    "\n",
    "        new_frame_time = time.time()                                                                        # Get time after finished processing current frame\n",
    "        fps = 1/(new_frame_time-prev_frame_time)                                                            # Calculate FPS as 1/(frame proceesing duration)\n",
    "        prev_frame_time = new_frame_time                                                                    # Save current time to be used in next frame\n",
    "        cv2.putText(final_img, \"FPS: \" + str(int(fps)), (20,30), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,0,0), 2)\n",
    "        \n",
    "        # Display the final annotated frame\n",
    "        cv2.imshow(\"YOLOv8 Players and Field Keypoints Detection with Team Prediction and Tactical Map\",    \n",
    "                    final_img)\n",
    "        \n",
    "        frame_filename = os.path.join(frame_save_path, f'frame_{frame_nbr:04d}.jpg')\n",
    "        cv2.imwrite(frame_filename, final_img)  # Save the frame as JPEG file\n",
    "\n",
    "        # Treat keyboard user inputs (\"p\" for pause/unpause & \"q\" for quit)\n",
    "        key = cv2.waitKey(1)\n",
    "        # Break the loop if 'q' is pressed\n",
    "        if key == ord(\"q\"):\n",
    "            break\n",
    "        if key == ord('p'):\n",
    "            cv2.waitKey(-1) #wait until any key is pressed\n",
    "    else:\n",
    "        # Break the loop if the end of the video is reached\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close the display window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "average_detections_per_class = {label: total / frame_nbr for label, total in class_detection_counts.items()}\n",
    "print(\"Average number of detections per class per frame:\", average_detections_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def create_video_from_frames(frame_folder, output_video_path, fps=30, codec='XVID'):\n",
    "    # Define the codec and create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*codec)\n",
    "    frame_files = sorted([os.path.join(frame_folder, f) for f in os.listdir(frame_folder) if f.endswith('.jpg')])\n",
    "    \n",
    "    # Read the first frame to get the size\n",
    "    first_frame = cv2.imread(frame_files[0])\n",
    "    height, width, layers = first_frame.shape\n",
    "    video = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "\n",
    "    for frame_file in frame_files:\n",
    "        frame = cv2.imread(frame_file)\n",
    "        video.write(frame)\n",
    "    \n",
    "    video.release()\n",
    "\n",
    "# Usage\n",
    "frame_folder = 'saved_frames'  # Folder containing all the frames\n",
    "output_video_path = 'output_video.avi'  # Path where the output video will be saved\n",
    "fps = 20  # Frames per second of the output video\n",
    "\n",
    "create_video_from_frames(frame_folder, output_video_path, fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cv2.__version__) #4.5.5.64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python==4.5.5.64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
